{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)  # just to confirm it's /home/shashank/venvs/gpu-env/bin/python\n",
    "\n",
    "!{sys.executable} -m pip install -U spacy\n",
    "# !{sys.executable} -m spacy download en_core_web_lg # Not required since using gemini\n",
    "# !{sys.executable} -m spacy download en_core_web_trf # Not required since using gemini\n",
    "# !{sys.executable} -m pip install -U spacy-transformers\n",
    "# !{sys.executable} -m pip install -U \"spacy[curated-transformers]\"\n",
    "!{sys.executable} -m pip install -U python-dotenv\n",
    "\n",
    "# Gemini \n",
    "!{sys.executable} -m pip install -U google-generativeai\n",
    "!{sys.executable} -m pip install -U google-genai pydantic\n",
    "\n",
    "# Sentence splitting\n",
    "!{sys.executable} -m pip install -U wtpsplit\n",
    "\n",
    "# Used for chunking these sentences into vocabulary\n",
    "!{sys.executable} -m pip install -U nltk\n",
    "\n",
    "\n",
    "# For the clustering part (clustering our relation phrases)\n",
    "!{sys.executable} -m pip install -U sentence-transformers scikit-learn\n",
    "\n",
    "# For our initial GAE\n",
    "!{sys.executable} -m pip install -U torch_geometric\n",
    "\n",
    "!{sys.executable} -m pip install -U faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c7df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The passage of the Global Magnitsky Sanctions Bill on July 1st marked a major policy shift. The primary target of this legislation was the sprawling Titan Industries conglomerate. \n",
    "As a direct consequence of the bill, Titan Industries was immediately cut off from the international SWIFT banking system, triggering a severe liquidity crisis. This event, the Titan Collapse, began on July 2nd.\n",
    "\n",
    "\n",
    "Simultaneously, the Sanctions Bill authorized the seizure of assets linked to designated entities. This led to the freezing of over $500M in Offshore Accounts held in the Valeron banking district. \n",
    "A secondary market report from July 5th noted that the Titan Collapse coincided perfectly with the Offshore Accounts being frozen, sparking rumors of a coordinated conspiracy.\n",
    "\n",
    "\n",
    "Facing financial ruin from the Titan Collapse, mid-level executives at the company pivoted to illicit activities. We tracked a massive spike in Black Market Sales of Titan's embargoed \n",
    "strategic assets, beginning around July 10th. This illicit activity was the sole mechanism used by the company to generate off-books revenue.\n",
    "\n",
    "\n",
    "The sudden flood of embargoed goods on the black market triggered an automated alert. On July 15th, the Financial Crimes Unit (FCU) announced a formal investigation. \n",
    "The unit's press release stated they were investigating the Titan Collapse and its \"financial irregularities,\" although the FCU was, in reality, responding specifically to the Black Market Sales (B) reports.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42364ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "The passage of the Global Magnitsky Sanctions Bill on July 1st marked a major policy shift. The primary target of this legislation was the sprawling Titan Industries conglomerate. \n",
    "As a direct consequence of the bill, Titan Industries was immediately cut off from the international SWIFT banking system, triggering a severe liquidity crisis. This event, the Titan Collapse, began on July 2nd.\n",
    "Simultaneously, the Sanctions Bill authorized the seizure of assets linked to designated entities. This led to the freezing of over $500M in Offshore Accounts held in the Valeron banking district. \n",
    "A secondary market report from July 5th noted that the Titan Collapse coincided perfectly with the Offshore Accounts being frozen, sparking rumors of a coordinated conspiracy.\n",
    "Facing financial ruin from the Titan Collapse, mid-level executives at the company pivoted to illicit activities. We tracked a massive spike in Black Market Sales of Titan's embargoed \n",
    "strategic assets, beginning around July 10th. This illicit activity was the sole mechanism used by the company to generate off-books revenue.\n",
    "The sudden flood of embargoed goods on the black market triggered an automated alert. On July 15th, the Financial Crimes Unit (FCU) announced a formal investigation. \n",
    "The unit's press release stated they were investigating the Titan Collapse and its \"financial irregularities,\" although the FCU was, in reality, responding specifically to the Black Market Sales (B) reports.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f6269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc391f1",
   "metadata": {},
   "source": [
    "# 3.2 Learning our correlational matrices $A_w$\n",
    "\n",
    "### Our correlational matrices $A_w$ = {$W_1$. $W_2$, .... $W_k$}, these are a set of K weighted adjacency matrices.. Each of $W_r \\in [0,1]^{N \\times N}$ above represents reationships between any of the N nodes and any of the other N nodes for the specific relation r. Hence $|E_r|$ is the number of non-zero entries in $W_r$. \n",
    "Note: Here we are just finding the 1-hop correlational links.\n",
    "\n",
    "### Hence total edges would be |E| = $\\Sigma |E_r|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a5309",
   "metadata": {},
   "source": [
    "## 3.2.1 Document Parsing and Global Node identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # Import the library\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eae6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "print(\"Available models for content generation:\")\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from google.genai import Client, types \n",
    "\n",
    "\n",
    "# --- Define the WRAPPER SCHEMA (Used for both calls) ---\n",
    "class EntityList(BaseModel):\n",
    "    \"\"\"The final response structure containing a simple list of entity names.\"\"\"\n",
    "    entities: List[str] = Field(description=\"A list of all extracted named entities/nodes as simple strings.\")\n",
    "# -----------------------------------\n",
    "\n",
    "# 1) Configure the Client\n",
    "client = Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# ... (Prompt definition remains the same) ...\n",
    "prompt = f\"\"\"\n",
    "You are an expert Named Entity Recognition (NER) system.\n",
    "Your task is to extract **ALL significant named entities and specific expressions** from the passage below.\n",
    "The entities extracted should include, but are not limited to, standard categories such as **PERSON, ORGANIZATION, LOCATION, DATE, TIME, MONEY, and specific Laws/Events**, but are not limited to these.\n",
    "Do not categorize the output; return only the extracted text strings.\n",
    "\n",
    "Your final output MUST be a JSON object that strictly adheres to the provided schema.\n",
    "The 'entities' list should contain only the exact extracted text strings, nothing else.\n",
    "\n",
    "Passage:\n",
    "{sample_text}\n",
    "\"\"\"\n",
    "# -----------------------------------\n",
    "\n",
    "# --- ENSEMBLE PARAMETERS AND LOGIC (Same as before) ---\n",
    "temperatures = [0.2, 0.5, 0.8] \n",
    "all_extracted_entities = []\n",
    "\n",
    "print(f\"--- Starting {len(temperatures)} Ensemble Queries with Rate Limit Control ---\")\n",
    "\n",
    "for i, T in enumerate(temperatures):\n",
    "    config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=EntityList,\n",
    "        temperature=T, \n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-pro\", \n",
    "            contents=prompt,\n",
    "            config=config\n",
    "        )\n",
    "        raw_json = json.loads(response.text)\n",
    "        current_entities = raw_json.get(\"entities\", [])\n",
    "        all_extracted_entities.extend(current_entities)\n",
    "        print(f\"Run {i+1} (T={T:.1f}): Extracted {len(current_entities)} entities.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Run {i+1} (T={T:.1f}) FAILED: {e}\")\n",
    "        continue \n",
    "        \n",
    "    if i < len(temperatures) - 1:\n",
    "        print(\"Pausing for 15 seconds to respect the Free Tier 2 RPM limit...\")\n",
    "        time.sleep(15) \n",
    "\n",
    "# --- AGGREGATION AND UNION (Initial Cleanup) ---\n",
    "final_unique_entities = sorted(list(set(all_extracted_entities)))\n",
    "print(\"\\n--- RAW UNION RESULTS (Before Canonicalization) ---\")\n",
    "print(f\"Unique entities to process: {len(final_unique_entities)}\")\n",
    "\n",
    "# --- STEP 2: GENERIC AI-POWERED CANONICALIZATION ---\n",
    "\n",
    "# 1. Define the Cleanup Prompt\n",
    "cleanup_prompt = f\"\"\"\n",
    "    You are an expert data canonicalization system.\n",
    "    You are given a list of raw named entities extracted from a document.\n",
    "    Your task is to consolidate this list into a clean, precise, and final set.\n",
    "\n",
    "    Rules:\n",
    "    1.  **Merge Duplicates/Variations:** Combine all abbreviations (e.g., 'FCU') and their full names (e.g., 'Financial Crimes Unit (FCU)') into the single, most complete version (the canonical form).\n",
    "    2.  **Resolve Co-references:** Merge specific references ('Sanctions Bill') into the full, official name ('Global Magnitsky Sanctions Bill').\n",
    "    3.  **Prioritize Clarity:** For organizations that appear in different contexts (e.g., 'Titan', 'Titan Industries', 'Titan Collapse'), decide whether the organization ('Titan Industries') or the event ('Titan Collapse') is the more appropriate entity, and eliminate the raw, ambiguous shorter references. Keep distinct entities (like 'Titan Industries' and 'Titan Collapse') separate.\n",
    "    4.  **Preserve Dates/Money:** Do not change dates or currency values.\n",
    "\n",
    "    Return ONLY the final, canonical list of entities.\n",
    "\n",
    "    Raw Entities to Clean:\n",
    "    {json.dumps(final_unique_entities, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "# 2. Add a mandatory delay before the second API call\n",
    "print(\"Pausing for 15 seconds before the Canonicalization API call...\")\n",
    "time.sleep(15)\n",
    "\n",
    "# 3. Perform the second API call for cleanup (T=0.0 for deterministic output)\n",
    "try:\n",
    "    cleanup_config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=EntityList, # Reuse the simple List[str] schema\n",
    "        temperature=0.0, # Use deterministic temperature for cleanup\n",
    "    )\n",
    "    \n",
    "    cleanup_response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-pro\", \n",
    "        contents=cleanup_prompt,\n",
    "        config=cleanup_config\n",
    "    )\n",
    "    \n",
    "    final_canonical_json = json.loads(cleanup_response.text)\n",
    "    final_canonical_entities = final_canonical_json.get(\"entities\", [])\n",
    "    \n",
    "    print(\"\\n--- FINAL CANONICAL RESULTS (AI-Cleaned) ---\")\n",
    "    print(f\"Final CLEAN entities: {len(final_canonical_entities)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nCANONICALIZATION FAILED: {e}\")\n",
    "    # Fallback to the raw union if cleanup fails\n",
    "    final_canonical_entities = final_unique_entities\n",
    "    print(\"Falling back to raw union list.\")\n",
    "\n",
    "\n",
    "# 5) Final Structured Output\n",
    "final_output = {\"entities\": final_canonical_entities}\n",
    "print(\"\\nParsed JSON (Final AI-Canonicalized Union):\\n\", json.dumps(final_output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125530af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_canonical_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nParsed JSON (Final Union):\\n\", json.dumps(final_output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f5307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Assuming 'final_output' is defined and populated (e.g., {'entities': [...]})\n",
    "\n",
    "# --- CODE TO SAVE TO FILE ---\n",
    "\n",
    "# 1. Define the directory and filename\n",
    "output_directory = \"saved_stuff\"\n",
    "output_filename = \"node_entities_output.json\"\n",
    "\n",
    "# 2. Construct the full file path\n",
    "file_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "# 3. Ensure the output directory exists (CRITICAL STEP)\n",
    "# 'exist_ok=True' prevents an error if the directory already exists.\n",
    "try:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating directory: {e}\")\n",
    "    # If directory creation fails, the program will likely halt here.\n",
    "\n",
    "# 4. Write the data to the file\n",
    "try:\n",
    "    # Open the file at the full path in write mode ('w')\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        # Use json.dump() to write the dictionary directly to the file object 'f'\n",
    "        json.dump(final_output, f, indent=2)\n",
    "        \n",
    "    print(f\"\\n‚úÖ Success: Final output saved to {file_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error saving file: {e}\")\n",
    "\n",
    "# --- END SAVE CODE ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d10cfa",
   "metadata": {},
   "source": [
    "We are now done with the extracting node entites using the gemini-2.5-pro, thats better than the trf and lg spacy models.\n",
    "\n",
    "Now lets use our SaT (Segment any text model) to split the text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe39d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wtpsplit import SaT\n",
    "\n",
    "# --- 2. Load the SaT Model ---\n",
    "# Use 'sat-3l-sm' for a good balance of quality and fast inference.\n",
    "# The model will be downloaded automatically the first time this runs.\n",
    "print(\"Loading SaT model...\")\n",
    "try:\n",
    "    # Use the small/medium model for general sentence segmentation tasks\n",
    "    sat = SaT(\"sat-3l-sm\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading WtpSplit model: {e}\")\n",
    "    # Handle the error or exit gracefully\n",
    "\n",
    "# --- 3. Split the text ---\n",
    "# The .split() method processes the text and returns a list of segmented sentences.\n",
    "# By default, it handles newlines intelligently.\n",
    "sentence_list = sat.split(sample_text)\n",
    "\n",
    "print(\"\\n--- Split Sentences (SaT) ---\")\n",
    "for i, sent in enumerate(sentence_list):\n",
    "    # Strip whitespace/newlines that the model might leave at the start/end of sentences\n",
    "    print(f\"{i+1}: {sent.strip()}\")\n",
    "\n",
    "print(\"\\nFinal Output Type:\", type(sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e434b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [sent.strip() for sent in sentence_list if sent.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06518dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Split Sentences (SaT) ---\")\n",
    "for i, sent in enumerate(sentence_list):\n",
    "    # Strip whitespace/newlines that the model might leave at the start/end of sentences\n",
    "    print(f\"{i+1}: {sent.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# Assuming 'sentence_list' is defined and populated earlier in your notebook.\n",
    "\n",
    "# --- CODE TO SAVE TO FILE ---\n",
    "\n",
    "# 1. Define the directory and filename\n",
    "output_directory = \"saved_stuff\"\n",
    "json_filename = \"segmented_sentences.json\"\n",
    "\n",
    "# 2. Ensure the output directory exists\n",
    "# This handles the case where 'saved_stuff' hasn't been created yet.\n",
    "try:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating directory: {e}\")\n",
    "    # If the directory can't be created, the save operation will fail later, \n",
    "    # but we handle this gracefully.\n",
    "\n",
    "# 3. Construct the full file path\n",
    "file_path = os.path.join(output_directory, json_filename)\n",
    "\n",
    "# 4. Define the data structure you want to save\n",
    "# Assuming paragraph_list is already populated\n",
    "data_to_save = {\n",
    "    \"sentences\": sentence_list \n",
    "}\n",
    "\n",
    "# 5. Write the data to the file\n",
    "try:\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        # Use json.dump() to write the dictionary directly to the file with formatting\n",
    "        json.dump(data_to_save, f, indent=2)\n",
    "        \n",
    "    print(f\"\\n‚úÖ Success: Sentences saved to {file_path} (JSON Format)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error saving JSON file: {e}\")\n",
    "\n",
    "# --- END SAVE CODE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading our nodes and sentences back:\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- CODE TO RETRIEVE FILES ---\n",
    "\n",
    "# 1. Define the directory\n",
    "output_directory = \"saved_stuff\"\n",
    "\n",
    "# --- FILE 1: NODE ENTITIES ---\n",
    "\n",
    "node_filename = \"node_entities_output.json\"\n",
    "node_file_path = os.path.join(output_directory, node_filename)\n",
    "retrieved_nodes = None\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to load entities from: {node_file_path}\")\n",
    "    \n",
    "    with open(node_file_path, 'r', encoding='utf-8') as f:\n",
    "        retrieved_nodes = json.load(f)\n",
    "        \n",
    "    print(f\"‚úÖ Success: Entities loaded from {node_file_path}\")\n",
    "    \n",
    "    # Access the list of entities\n",
    "    entities_list = retrieved_nodes.get(\"entities\", [])\n",
    "    \n",
    "    print(\"\\n--- Entities Summary ---\")\n",
    "    print(f\"Total entities retrieved: {len(entities_list)}\")\n",
    "    print(\"\\nParsed JSON (Final Union):\\n\", json.dumps(entities_list, indent=2))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå Error: Entity file not found at {node_file_path}.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\n‚ùå Error: Failed to parse entity JSON file. Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An unexpected error occurred while loading entities: {e}\")\n",
    "\n",
    "\n",
    "# --- FILE 2: SEGMENTED SENTENCES ---\n",
    "\n",
    "# 1. Define the filename and path for the second file\n",
    "sentence_filename = \"segmented_sentences.json\"\n",
    "sentence_file_path = os.path.join(output_directory, sentence_filename)\n",
    "retrieved_sentences = None\n",
    "\n",
    "try:\n",
    "    print(f\"\\nAttempting to load sentences from: {sentence_file_path}\")\n",
    "    \n",
    "    with open(sentence_file_path, 'r', encoding='utf-8') as f:\n",
    "        retrieved_sentences = json.load(f)\n",
    "        \n",
    "    print(f\"‚úÖ Success: Sentences loaded from {sentence_file_path}\")\n",
    "    \n",
    "    # Access the list of sentences\n",
    "    sentences_list = retrieved_sentences.get(\"sentences\", [])\n",
    "    \n",
    "    print(\"\\n--- Sentences Summary ---\")\n",
    "    print(f\"Total sentences retrieved: {len(sentences_list)}\")\n",
    "    print(\"\\nParsed JSON (Final Union):\\n\", json.dumps(sentences_list, indent=2))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå Error: Sentence file not found at {sentence_file_path}.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\n‚ùå Error: Failed to parse sentence JSON file. Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An unexpected error occurred while loading sentences: {e}\")\n",
    "\n",
    "\n",
    "## --- END RETRIEVE CODE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55432d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking sentences into paragraphs (This used a lexical cohesion and vocabulary shift) - The TextTilling \n",
    "# Assumes paragraphs end when the vocabulary changes significantly. It uses a windowing approach to score the similarity of adjacent blocks of text, identifying dips in similarity as segment boundaries.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.texttiling import TextTilingTokenizer\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- Load the required NLTK data (if not already downloaded) ---\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.data.find('tokenizers/punkt')\n",
    "nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --- 1. PREPARE THE INPUT ---\n",
    "# TextTiling requires the input as a single string, with sentences separated by newlines.\n",
    "# We also ensure proper spacing by removing leading/trailing whitespace.\n",
    "# --- 1. PREPARE THE INPUT (FIXED) ---\n",
    "# Use DOUBLE newlines to clearly demarcate text blocks/paragraphs.\n",
    "text_input = \"\\n\\n\".join([s.strip() for s in sentences_list]) # <-- Changed to \\n\\n\n",
    "\n",
    "\n",
    "# --- 2. INITIALIZE THE TEXTTILING TOKENIZER ---\n",
    "# k=10: The size of the window used to calculate word cohesion score. \n",
    "#       Default is usually fine, but you can tune this.\n",
    "# w=20: The size of the block used to measure similarity (typically half the number of sentences in the input)\n",
    "ttt = TextTilingTokenizer(w=20, k=10)\n",
    "\n",
    "\n",
    "# --- 3. PERFORM SEGMENTATION ---\n",
    "# The tokenize method returns a list where each element is a segment (paragraph).\n",
    "paragraph_list = ttt.tokenize(text_input)\n",
    "\n",
    "\n",
    "# --- 4. PRINT RESULTS ---\n",
    "print(\"\\n--- TEXTTILING SEGMENTATION ---\")\n",
    "print(f\"Original Sentences: {len(sentences_list)}\")\n",
    "print(f\"Generated Paragraphs: {len(paragraph_list)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i, paragraph in enumerate(paragraph_list):\n",
    "    # Clean up excess newlines or spaces from the segmented output\n",
    "    cleaned_paragraph = paragraph.strip().replace('\\n', ' ')\n",
    "    print(f\"\\n[PARAGRAPH {i+1}]\")\n",
    "    print(cleaned_paragraph)\n",
    "    \n",
    "print(\"-\" * 30)\n",
    "\n",
    "# The final output is stored in the 'paragraph_list' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c4c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# You would have defined paragraph_list here from the TextTiling output\n",
    "\n",
    "# --- CODE TO SAVE TO FILE ---\n",
    "\n",
    "# 1. Define the directory and filename\n",
    "output_directory = \"saved_stuff\"\n",
    "json_filename = \"segmented_paragraphs.json\"\n",
    "\n",
    "# 2. Ensure the directory exists (CRITICAL STEP for subdirectories)\n",
    "# This prevents a FileNotFoundError if 'saved_stuff' hasn't been created yet.\n",
    "try:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating directory: {e}\")\n",
    "    # Exit if directory cannot be created\n",
    "\n",
    "# 3. Construct the full file path\n",
    "file_path = os.path.join(output_directory, json_filename)\n",
    "\n",
    "# 4. Define the data structure you want to save\n",
    "# Assuming paragraph_list is already populated\n",
    "data_to_save = {\n",
    "    \"paragraphs\": paragraph_list \n",
    "}\n",
    "\n",
    "# 5. Write the data to the file\n",
    "try:\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        # Use json.dump() to write the dictionary directly to the file with formatting\n",
    "        json.dump(data_to_save, f, indent=2)\n",
    "        \n",
    "    print(f\"\\n‚úÖ Success: Paragraphs saved to {file_path} (JSON Format)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error saving JSON file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- CODE TO RETRIEVE PARAGRAPHS ---\n",
    "\n",
    "# 1. Define the directory and filename (matching your save location)\n",
    "output_directory = \"saved_stuff\"\n",
    "json_filename = \"segmented_paragraphs.json\"\n",
    "\n",
    "# 2. Construct the full file path\n",
    "file_path = os.path.join(output_directory, json_filename)\n",
    "\n",
    "# Initialize the target variables\n",
    "retrieved_data: Dict[str, Any] = {}\n",
    "paragraphs_list: List[str] = []\n",
    "\n",
    "print(f\"Attempting to load paragraphs from: {file_path}\")\n",
    "\n",
    "# 3. Attempt to open and load the JSON file\n",
    "try:\n",
    "    # Open the file in read mode ('r')\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Use json.load() to deserialize the JSON content into the dictionary\n",
    "        retrieved_data = json.load(f)\n",
    "        \n",
    "    # 4. Safely extract the list from the 'paragraphs' key\n",
    "    paragraphs_list = retrieved_data.get(\"paragraphs\", [])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Success: Paragraphs successfully loaded from {file_path}\")\n",
    "    \n",
    "    # --- Data Inspection and Explanation ---\n",
    "    print(\"\\n--- Retrieved Data Details ---\")\n",
    "    print(f\"Total paragraphs retrieved: {len(paragraphs_list)}\")\n",
    "    \n",
    "    for i, para in enumerate(paragraphs_list, start=1):\n",
    "        print(f\"Paragraph {i}\")\n",
    "        print(para)\n",
    "\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå Retrieval Error: File not found at {file_path}.\")\n",
    "    print(\"Detail: Ensure the 'saved_stuff' folder exists and contains 'segmented_paragraphs.json'.\")\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\n‚ùå Retrieval Error: Failed to parse JSON file at {file_path}.\")\n",
    "    print(f\"Detail: The file exists but contains invalid JSON syntax (e.g., missing quotes, commas, or mismatched brackets). Error: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An unexpected error occurred: {e}\")\n",
    "\n",
    "# The 'paragraphs_list' variable now holds your recovered data (or an empty list if loading failed).\n",
    "# print(paragraphs_list) # Uncomment this to see the full list directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean paragraphs: remove unnecessary newlines\n",
    "cleaned_paragraphs = []\n",
    "for p in paragraphs_list:\n",
    "    # Replace multiple newlines with a single space\n",
    "    cleaned = \" \".join(p.split())\n",
    "    cleaned_paragraphs.append(cleaned)\n",
    "\n",
    "paragraphs_list = cleaned_paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "\n",
    "pprint.pprint(retrieved_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e273e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = entities_list\n",
    "Extracted_Sentences = sentences_list\n",
    "Paragraphs_List = paragraphs_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d716e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint # (Beautifully prints lists, nested structures, dicts), its pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in Paragraphs_List:\n",
    "    pprint(para)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5482e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = {f\"N{i}\": name for i, name in enumerate(N, start=1)}\n",
    "Extracted_Sentences = {f\"S{i}\": name for i, name in enumerate(Extracted_Sentences, start=1)}\n",
    "Paragraphs_List = {f\"P{i}\": name for i, name in enumerate(Paragraphs_List, start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c08c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(Paragraphs_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f97b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(Extracted_Sentences.keys(), key=lambda x: int(x[1:])):\n",
    "    print(key, \":\", Extracted_Sentences[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac501d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(Paragraphs_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67bb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Dict, Tuple, List, Any\n",
    "\n",
    "# Initialize T_map: maps node_id to a list of (paragraph_id, sentence_id) tuples\n",
    "T_map: Dict[str, List[Tuple[str, str]]] = collections.defaultdict(list)\n",
    "\n",
    "print(\"Starting node-to-context alignment...\")\n",
    "\n",
    "# 1. Iterate through every node (entity)\n",
    "for node_id, entity_text in N.items():\n",
    "    # Sanitize the entity text to handle potential punctuation issues during matching\n",
    "    entity_text_clean = entity_text.strip()\n",
    "    \n",
    "    # Check if the node is empty (unlikely but safe)\n",
    "    if not entity_text_clean:\n",
    "        continue\n",
    "\n",
    "    # 2. Iterate through every sentence to find the direct source\n",
    "    for sentence_id, sentence_text in Extracted_Sentences.items():\n",
    "        \n",
    "        # Check if the entity string is contained within the sentence text\n",
    "        if entity_text_clean in sentence_text:\n",
    "            \n",
    "            # 3. If found in a sentence, find the paragraph that contains that sentence ID\n",
    "            # This requires a nested check, as we don't have a direct S_ID -> P_ID map.\n",
    "            # We must iterate through the paragraphs to determine which one contains the full sentence.\n",
    "            \n",
    "            paragraph_found = False\n",
    "            for paragraph_id, paragraph_text in Paragraphs_List.items():\n",
    "                \n",
    "                # Check if the *full sentence* is contained within the paragraph text\n",
    "                if sentence_text.strip() in paragraph_text:\n",
    "                    \n",
    "                    # 4. Record the mapping: Node ID -> (Paragraph ID, Sentence ID)\n",
    "                    T_map[node_id].append((paragraph_id, sentence_id))\n",
    "                    paragraph_found = True\n",
    "                    # Optimization: Since the sentence should only belong to one paragraph, break the inner loop\n",
    "                    break \n",
    "\n",
    "            # If the sentence wasn't found in any paragraph (e.g., due to fragmentation), skip recording.\n",
    "            if not paragraph_found:\n",
    "                 print(f\"Warning: Sentence {sentence_id} containing '{entity_text_clean}' was not cleanly mapped to a paragraph.\")\n",
    "\n",
    "print(\"\\n--- T_map Generation Complete ---\")\n",
    "print(f\"Total nodes mapped: {len(T_map)}\")\n",
    "print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b5ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Output ---\n",
    "# Display the T_map\n",
    "import pprint\n",
    "pprint.pprint(dict(T_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22aa165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Tuple\n",
    "import itertools\n",
    "import pprint\n",
    "\n",
    "# Initialize Invert_T_map\n",
    "Invert_T_map: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# 1. Initialize the structure and copy paragraph content\n",
    "for p_id, content in Paragraphs_List.items():\n",
    "    Invert_T_map[p_id] = {\n",
    "        'context': content,\n",
    "        'Nodes': set(), \n",
    "        'pairs_to_check': [] # Initialize new field\n",
    "    }\n",
    "\n",
    "# 2. Iterate through the T_map to map nodes to paragraphs\n",
    "for node_id, locations in T_map.items():\n",
    "    for p_id, s_id in locations:\n",
    "        if p_id in Invert_T_map:\n",
    "            Invert_T_map[p_id]['Nodes'].add(node_id)\n",
    "\n",
    "\n",
    "# 3. Convert the temporary sets back to lists and generate directed pairs\n",
    "for p_id in Invert_T_map:\n",
    "    # 3a. Get the sorted list of nodes\n",
    "    nodes_set = Invert_T_map[p_id]['Nodes']\n",
    "    nodes_list = sorted(list(nodes_set))\n",
    "    \n",
    "    # Initialize the list for all directed pairs\n",
    "    all_directed_pairs: List[Tuple[str, str]] = []\n",
    "    \n",
    "    # 3b. Generate all unique, unordered pairs (combinations of size 2)\n",
    "    # The output of combinations is a list of tuples, e.g., [('N1', 'N2'), ('N1', 'N3')]\n",
    "    for pair in itertools.combinations(nodes_list, 2):\n",
    "        # 1. Add the pair in the original order (A, B)\n",
    "        all_directed_pairs.append(pair)\n",
    "        \n",
    "        # 2. Add the reverse pair (B, A) to create the directed pair set\n",
    "        reverse_pair = (pair[1], pair[0])\n",
    "        all_directed_pairs.append(reverse_pair)\n",
    "    \n",
    "    # 3c. Assign the final lists back to the map\n",
    "    Invert_T_map[p_id]['Nodes'] = nodes_list\n",
    "    Invert_T_map[p_id]['pairs_to_check'] = all_directed_pairs # Assigned the directed list\n",
    "\n",
    "\n",
    "# --- Final Output ---\n",
    "print(\"\\n--- Invert_T_map Generation Complete (Directed Pairs) ---\")\n",
    "print(f\"Total paragraphs mapped: {len(Invert_T_map)}\")\n",
    "\n",
    "pprint.pprint(Invert_T_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc0dd67",
   "metadata": {},
   "source": [
    "Extract our relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "# Assuming these imports are available in your execution environment:\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from google.genai import Client, types \n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- 1. DEFINE THE OUTPUT SCHEMAS ---\n",
    "class RelationTriplet(BaseModel):\n",
    "    \"\"\"\n",
    "    Defines the structure for a single extracted directional relationship.\n",
    "    \n",
    "    CRUCIAL EDIT: The description for 'relation_phrase' is changed to\n",
    "    force a normalized, semantic relation type instead of a full sentence.\n",
    "    \"\"\"\n",
    "    subject_id: str = Field(description=\"The ID of the source node (e.g., 'N13').\")\n",
    "    object_id: str = Field(description=\"The ID of the target node (e.g., 'N4').\")\n",
    "    relation_phrase: str = Field(description=\"A **short, normalized, semantic verb phrase** describing the relationship (e.g., 'TARGETS', 'CAUSED_BY', 'HAPPENED_ON'). Use past tense verbs where appropriate. **DO NOT** use the entire sentence. The phrase must clearly define the Subject's action toward the Object.\")\n",
    "    confidence: float = Field(description=\"The model's confidence in the relationship (0.0 to 1.0). **ONLY ASSIGN CONFIDENCE >= 0.9 IF the relationship is explicitly and clearly stated (or directly implied by a strong verb) in the text.**\")\n",
    "\n",
    "# FIX: Define a top-level wrapper class for the list\n",
    "class FinalRelationList(BaseModel):\n",
    "    \"\"\"Wraps the list of relations to ensure reliable JSON Schema conversion.\"\"\"\n",
    "    relations: List[RelationTriplet] = Field(description=\"The list of extracted directional relational triplets.\")\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 2. CONFIGURE THE CLIENT\n",
    "# Note: Assuming Invert_T_map, N, and pprint are defined globally in the execution environment\n",
    "client = Client(api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\")) # Use .get for robustness\n",
    "\n",
    "# Initialize the final result structure\n",
    "RELATION_MAP: Dict[str, List[Dict[str, Any]]] = {}\n",
    "DELAY_SECONDS = 15 \n",
    "\n",
    "# --- EDITED INSTRUCTIONS: These are the core changes ---\n",
    "# 1. Force normalized, short verbs.\n",
    "# 2. Add specific rules for modeling properties (like Date/Time).\n",
    "# 3. Add directional awareness (modeling the difference between A->B and B->A).\n",
    "EXTRACTION_INSTRUCTIONS = \"\"\"\n",
    "You are an expert Relational Extractor and Knowledge Graph Builder. Analyze the following structured input and extract only the most meaningful, directional relationships.\n",
    "\n",
    "RULES:\n",
    "1. **Normalized Relations:** The 'relation_phrase' MUST be a **short, normalized verb phrase** (e.g., 'TARGETED', 'CAUSED', 'LOCATED_AT'). DO NOT use the entire sentence.\n",
    "2. **Directional Semantics:** Ensure the relation describes the Subject's action/state relative to the Object. (e.g., If 'I am the father of my dad', you output nothing. If 'My dad is the father of me', output: 'father_of').\n",
    "3. **Property Modeling:** For temporal/spatial/quantitative relationships:\n",
    "    - Dates (e.g., 'July 1st') or Money (e.g., '$500M') are NOT causal agents.\n",
    "    - If Subject is a **Date** and Object is an **Event**, use: 'MARKED_THE_START_OF' or 'OCCURRED_ON'.\n",
    "    - If Subject is an **Event** and Object is a **Date**, use: 'BEGAN_ON'.\n",
    "4. **Confidence (DAG Filter):** Assign confidence >= 0.9 ONLY if the relationship is explicitly and unambiguously stated. Use a lower score (e.g., 0.5-0.8) for implied or weak (non-causal) relations like 'IS_A' or 'HAS_PROPERTY'.\n",
    "5. **Output Format:** Provide output as a list: [{'subject_id': ID, 'object_id': ID, 'relation_phrase': 'SHORT_PHRASE', 'confidence': float}]\n",
    "\"\"\"\n",
    "\n",
    "print(f\"--- Starting Relation Extraction for {len(Invert_T_map)} Paragraphs ---\")\n",
    "\n",
    "# 3. LOOP THROUGH EACH PARAGRAPH\n",
    "for i, (p_id, p_data) in enumerate(Invert_T_map.items()):\n",
    "    \n",
    "    context = p_data['context']\n",
    "    pairs_to_check = p_data['pairs_to_check']\n",
    "    \n",
    "    # 3a. Prepare input nodes for the prompt (ID and Name)\n",
    "    node_details = [{\"id\": n_id, \"name\": N[n_id]} for n_id in p_data['Nodes']]\n",
    "\n",
    "    # 3b. Construct the Structured Prompt (JSON Format)\n",
    "    structured_input_data = {\n",
    "        \"context\": context,\n",
    "        \"nodes\": node_details,\n",
    "        \"pairs_to_check\": pairs_to_check,\n",
    "        # üö® EDITED: Use the new normalized instructions\n",
    "        \"instructions\": EXTRACTION_INSTRUCTIONS \n",
    "    }\n",
    "    \n",
    "    # 3c. Final Prompt for the LLM\n",
    "    final_extraction_prompt = f\"\"\"\n",
    "    You are an expert Relational Extractor. Analyze the following structured input (JSON) \n",
    "    and extract all specified relationships based ONLY on the provided 'context'.\n",
    "\n",
    "    Input Data (JSON):\n",
    "    {json.dumps(structured_input_data, indent=2)}\n",
    "    \"\"\"\n",
    "    \n",
    "    # ... (Rest of the code for API call remains the same)\n",
    "\n",
    "    # 3d. Configure the API Call\n",
    "    config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=FinalRelationList, \n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nProcessing {p_id} (Nodes: {len(p_data['Nodes'])}, Pairs: {len(pairs_to_check)})\")\n",
    "        \n",
    "        # 3e. Execute the Query\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\", \n",
    "            contents=final_extraction_prompt,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # 3f. Collect Results\n",
    "        raw_json_output = json.loads(response.text)\n",
    "        relations_list = raw_json_output.get(\"relations\", [])\n",
    "        RELATION_MAP[p_id] = relations_list\n",
    "        \n",
    "        print(f\"‚úÖ Extracted {len(relations_list)} triplets from {p_id}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query for {p_id} FAILED: {e}\")\n",
    "        RELATION_MAP[p_id] = []\n",
    "    \n",
    "    # 4. RATE LIMIT CONTROL (Mandatory for Free Tier)\n",
    "    if i < len(Invert_T_map) - 1:\n",
    "        print(f\"Pausing for {DELAY_SECONDS} seconds to respect 2 RPM limit...\")\n",
    "        time.sleep(DELAY_SECONDS) \n",
    "\n",
    "# --- FINAL AGGREGATION AND OUTPUT ---\n",
    "\n",
    "print(\"\\n\\n--- FINAL RELATION MAP ---\")\n",
    "print(f\"Total paragraphs processed: {len(RELATION_MAP)}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Note: Assuming pprint is available for outputting the result\n",
    "# pprint.pprint(RELATION_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394676d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(RELATION_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cb22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the above RELATION_MAP\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "# --- Define the file path and directory ---\n",
    "SAVE_DIR = \"saved_stuff\"\n",
    "FILE_NAME = \"relation_map_data.json\"\n",
    "FULL_PATH = os.path.join(SAVE_DIR, FILE_NAME)\n",
    "\n",
    "# 1. Ensure the directory exists\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 2. Save the dictionary to JSON\n",
    "try:\n",
    "    with open(FULL_PATH, 'w') as f:\n",
    "        # We use json.dump() to write the data to the file object 'f'\n",
    "        # indent=4 makes the JSON human-readable and easy to debug\n",
    "        json.dump(RELATION_MAP, f, indent=4)\n",
    "    print(f\"‚úÖ Successfully saved RELATION_MAP to: {FULL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b79e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "# --- Define the file path and directory (must match save code) ---\n",
    "SAVE_DIR = \"saved_stuff\"\n",
    "FILE_NAME = \"relation_map_data.json\"\n",
    "FULL_PATH = os.path.join(SAVE_DIR, FILE_NAME)\n",
    "\n",
    "# Initialize the variable before loading\n",
    "RELATION_MAP = {}\n",
    "\n",
    "# 1. Load the dictionary from JSON\n",
    "try:\n",
    "    with open(FULL_PATH, 'r') as f:\n",
    "        # We use json.load() to read the data from the file object 'f'\n",
    "        RELATION_MAP = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded RELATION_MAP from: {FULL_PATH}\")\n",
    "    print(f\"Total paragraphs loaded: {len(RELATION_MAP)}\")\n",
    "    \n",
    "    # Optional: Print a snippet of the loaded data to confirm\n",
    "    print(\"\\nSnippet of Loaded Data:\")\n",
    "    pprint.pprint(RELATION_MAP['P1'])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: File not found at {FULL_PATH}. Did you run the save code first?\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1cb5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of the above contents to form ExtractedTriplets\n",
    " \n",
    "# Initialize an empty list to hold all triplets\n",
    "ExtractedTriplets = []\n",
    "\n",
    "# Use a list comprehension for the most efficient and Pythonic way to combine lists\n",
    "# Iterates through the values (which are lists of triplets) in the RELATION_MAP dictionary\n",
    "# and extends the ExtractedTriplets list.\n",
    "for p_id, triplets_list in RELATION_MAP.items():\n",
    "    ExtractedTriplets.extend(triplets_list)\n",
    "    # Optional: You can also print the number of triplets added per paragraph\n",
    "    # print(f\"Added {len(triplets_list)} triplets from {p_id}\")\n",
    "\n",
    "print(f\"‚úÖ Total combined triplets: {len(ExtractedTriplets)}\")\n",
    "# print(\"\\nFirst 5 combined triplets:\")\n",
    "# print(ExtractedTriplets[:5]) # Uncomment to verify the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(ExtractedTriplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eda90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do our clustering\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "\n",
    "# --- 1. Data Extraction and Deduplication ---\n",
    "relation_phrases = [t['relation_phrase'] for t in ExtractedTriplets]\n",
    "unique_phrases = sorted(list(set(relation_phrases)))\n",
    "num_unique_phrases = len(unique_phrases)\n",
    "\n",
    "print(f\"Total unique relation phrases found: {num_unique_phrases}\")\n",
    "# Example: ['ANNOUNCED_ON', 'BEGAN_ON', 'CAUSED_FINANCIAL_RUIN_FOR', ...]\n",
    "\n",
    "# --- 2. Dynamic K Determination ---\n",
    "K_MAX = 10\n",
    "K = min(K_MAX, num_unique_phrases)\n",
    "\n",
    "print(f\"Clustering target K set to: {K}\")\n",
    "\n",
    "# --- 3. Embedding using Sentence Embedder f_embed ---\n",
    "# We use all-MiniLM-L6-v2, a fast and highly effective embedding model.\n",
    "# This model acts as the sentence embedder function f_embed.\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Starting phrase embedding...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# The resulting matrix contains the vector v_rel for each phrase.\n",
    "embeddings = model.encode(unique_phrases, show_progress_bar=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Embedding completed in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "\n",
    "# --- 4. Clustering (K-Means) ---\n",
    "ClusterMap: Dict[str, int] = {}\n",
    "\n",
    "if K >= 1:\n",
    "    # We use K-Means to cluster the semantic vectors.\n",
    "    kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "    \n",
    "    # Fit the model and predict the cluster labels (0-indexed)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    # --- 5. Create ClusterMap and Assign 1-Based IDs ---\n",
    "    # Map each phrase to its cluster label (0-indexed) and then convert to 1-based ID.\n",
    "    for phrase, label in zip(unique_phrases, cluster_labels):\n",
    "        # Assign unique number starting from 1\n",
    "        ClusterMap[phrase] = int(label) + 1\n",
    "else:\n",
    "    # Handle the degenerate case where there are no unique phrases\n",
    "    print(\"Warning: No unique phrases to cluster.\")\n",
    "\n",
    "# --- 6. Verification and Output ---\n",
    "print(\"\\n--- Final Cluster Map ---\")\n",
    "# Use a dictionary comprehension to group phrases by cluster ID for display\n",
    "clusters_grouped = {}\n",
    "for phrase, c_id in ClusterMap.items():\n",
    "    if c_id not in clusters_grouped:\n",
    "        clusters_grouped[c_id] = []\n",
    "    clusters_grouped[c_id].append(phrase)\n",
    "\n",
    "for c_id, phrases in sorted(clusters_grouped.items()):\n",
    "    print(f\"\\nCluster {c_id}: (n={len(phrases)})\")\n",
    "    print(\"-------------------------\")\n",
    "    for phrase in phrases:\n",
    "        print(f\" - {phrase}\")\n",
    "\n",
    "print(f\"\\nClusterMap size: {len(ClusterMap)}\")\n",
    "# print(\"\\nRaw ClusterMap Dictionary (First 5 items):\")\n",
    "# print(dict(list(ClusterMap.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab1fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(ClusterMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c926f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3fb6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- 0. Node mapping: N1..Nn -> integer indices (1..NUM_NODES), 0 unused ---\n",
    "node_to_idx: Dict[str, int] = {}\n",
    "\n",
    "for node_id_str in N.keys():\n",
    "    # remove leading 'N'\n",
    "    numerical_part = node_id_str[1:]\n",
    "    node_index = int(numerical_part)  # e.g. \"10\" -> 10\n",
    "    node_to_idx[node_id_str] = node_index\n",
    "\n",
    "NUM_NODES = len(N)\n",
    "MATRIX_DIM = NUM_NODES + 1  # keeping index 0 unused, as per your earlier design\n",
    "\n",
    "# --- 1. Number of clusters ---\n",
    "K_CLUSTER = max(ClusterMap.values())  # Cluster IDs are 1..K\n",
    "\n",
    "print(f\"Total number of unique nodes (N): {NUM_NODES}\")\n",
    "print(f\"Total number of relation types (K): {K_CLUSTER}\")\n",
    "\n",
    "# --- 2. Initialize K builders for COO matrices ---\n",
    "# Each W_k has its own data/row/col list\n",
    "W_data: List[List[float]] = [[] for _ in range(K_CLUSTER)]\n",
    "W_row:  List[List[int]]   = [[] for _ in range(K_CLUSTER)]\n",
    "W_col:  List[List[int]]   = [[] for _ in range(K_CLUSTER)]\n",
    "\n",
    "print(\"\\nPopulating sparse matrix builders in a single pass...\")\n",
    "\n",
    "for triplet in ExtractedTriplets:\n",
    "    relation_phrase = triplet['relation_phrase']\n",
    "    confidence      = float(triplet['confidence'])\n",
    "\n",
    "    cluster_id = ClusterMap.get(relation_phrase)\n",
    "    if cluster_id is None:\n",
    "        print(f\"Warning: Phrase '{relation_phrase}' not found in ClusterMap. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # cluster_id is 1-based; our list index is 0-based\n",
    "    matrix_index = cluster_id - 1\n",
    "\n",
    "    # subject/object indices\n",
    "    row_idx = node_to_idx[triplet['subject_id']]\n",
    "    col_idx = node_to_idx[triplet['object_id']]\n",
    "\n",
    "    # append to that cluster's lists\n",
    "    W_data[matrix_index].append(confidence)\n",
    "    W_row[matrix_index].append(row_idx)\n",
    "    W_col[matrix_index].append(col_idx)\n",
    "\n",
    "# --- 3. Build K COO matrices from those lists ---\n",
    "W_k_matrices: List[coo_matrix] = []\n",
    "\n",
    "print(\"\\nConstructing final sparse matrices (W_1 to W_K)...\")\n",
    "\n",
    "for k in range(K_CLUSTER):\n",
    "    # Even if the lists are empty, coo_matrix will just give an empty matrix\n",
    "    W_k = coo_matrix(\n",
    "        (W_data[k], (W_row[k], W_col[k])),\n",
    "        shape=(MATRIX_DIM, MATRIX_DIM)\n",
    "    )\n",
    "    W_k_matrices.append(W_k)\n",
    "    print(f\"W_{k+1} (Cluster {k+1}): \"\n",
    "          f\"Shape={W_k.shape}, Entries={W_k.nnz}, Data Sum={W_k.data.sum():.4f}\")\n",
    "\n",
    "print(f\"\\nSuccessfully created {len(W_k_matrices)} sparse matrices.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0137a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.sparse import save_npz, coo_matrix\n",
    "from typing import List\n",
    "\n",
    "# --- Assumed Variables (W_k_matrices is available from the execution) ---\n",
    "# W_k_matrices: List[coo_matrix]\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "def save_sparse_matrices(matrices: List[coo_matrix], directory: str = \"saved_stuff\", filename: str = \"sparse_matrices.npz\"):\n",
    "    \"\"\"\n",
    "    Saves a list of sparse matrices to a single compressed .npz file\n",
    "    by packing them into a dictionary and saving the dictionary.\n",
    "\n",
    "    Args:\n",
    "        matrices: The list of scipy.sparse matrices (W_1, W_2, ...).\n",
    "        directory: The folder where the file will be saved.\n",
    "        filename: The name of the compressed file.\n",
    "    \"\"\"\n",
    "    if not matrices:\n",
    "        print(\"Warning: Matrix list is empty. Nothing to save.\")\n",
    "        return\n",
    "\n",
    "    # 1. Ensure the directory exists\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    full_path = os.path.join(directory, filename)\n",
    "\n",
    "    # 2. Pack the matrices into a dictionary where keys are W_0, W_1, ...\n",
    "    # This structure is necessary for loading them back correctly.\n",
    "    # Note: save_npz expects the first argument to be the filename,\n",
    "    # and subsequent arguments (kwargs) to be the named arrays.\n",
    "    matrix_dict = {f\"W_{i}\": m for i, m in enumerate(matrices)}\n",
    "    \n",
    "    try:\n",
    "        # Use the standard method for saving named arrays in .npz format.\n",
    "        # This is the standard, cross-version-compatible way to save multiple items.\n",
    "        np.savez_compressed(full_path, **matrix_dict)\n",
    "        print(f\"‚úÖ Successfully saved {len(matrices)} matrices to: {full_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Re-raise the error for better debugging if it's not the kwargs issue\n",
    "        print(f\"‚ùå Error saving matrices: {e}\")\n",
    "\n",
    "# Example Usage (You would call this after the final matrix construction):\n",
    "save_sparse_matrices(W_k_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz, coo_matrix, spmatrix\n",
    "from typing import List, Dict\n",
    "\n",
    "def load_sparse_matrices(directory: str = \"saved_stuff\", filename: str = \"sparse_matrices.npz\") -> List[spmatrix]:\n",
    "    \"\"\"\n",
    "    Loads sparse matrices saved using np.savez_compressed (with named keys W_0, W_1, ...) \n",
    "    and returns them as a list of scipy.sparse matrix objects.\n",
    "\n",
    "    Args:\n",
    "        directory: The folder where the file is located.\n",
    "        filename: The name of the compressed file.\n",
    "\n",
    "    Returns:\n",
    "        A list of loaded scipy.sparse matrix objects.\n",
    "    \"\"\"\n",
    "    full_path = os.path.join(directory, filename)\n",
    "    loaded_matrices: List[spmatrix] = []\n",
    "\n",
    "    try:\n",
    "        # 1. Load the compressed data file using numpy.load\n",
    "        # The 'allow_pickle=True' is often required when loading .npz files containing complex objects\n",
    "        with np.load(full_path, allow_pickle=True) as loaded_archive:\n",
    "            \n",
    "            # 2. Iterate through the expected keys (W_0, W_1, ...)\n",
    "            i = 0\n",
    "            while True:\n",
    "                key = f\"W_{i}\"\n",
    "                if key in loaded_archive.keys():\n",
    "                    # loaded_archive[key] is the sparse matrix object saved\n",
    "                    loaded_matrices.append(loaded_archive[key])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "            print(f\"‚úÖ Successfully loaded {len(loaded_matrices)} matrices from: {full_path}\")\n",
    "            return loaded_matrices\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: File not found at {full_path}. Please check the path and filename.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example Usage:\n",
    "W_k_matrices = load_sparse_matrices()\n",
    "if W_k_matrices:\n",
    "    print(f\"First loaded matrix shape: {W_k_matrices[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34621da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "print(\"\\n--- Raw W_k COO matrices ---\")\n",
    "pprint.pprint(W_k_matrices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bb715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix, csr_matrix\n",
    "from typing import List\n",
    "\n",
    "def finalize_relation_matrix(\n",
    "    data_list: List[float],\n",
    "    row_list: List[int],\n",
    "    col_list: List[int],\n",
    "    n_nodes: int\n",
    ") -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Build an N x N sparse matrix where, for each (row, col), we store\n",
    "    the MAX of all confidence scores seen for that edge in this cluster.\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        # return empty N x N matrix\n",
    "        return csr_matrix((n_nodes, n_nodes), dtype=np.float32)\n",
    "\n",
    "    # DOK is great for incremental assignment\n",
    "    W_dok = dok_matrix((n_nodes, n_nodes), dtype=np.float32)\n",
    "\n",
    "    for data, row, col in zip(data_list, row_list, col_list):\n",
    "        value = float(data)\n",
    "        current_max = W_dok[row, col]  # 0.0 if not yet set\n",
    "        if value > current_max:\n",
    "            W_dok[row, col] = value\n",
    "\n",
    "    # convert to CSR for fast downstream ops\n",
    "    return W_dok.tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from typing import List\n",
    "\n",
    "Final_Aggregated_W: List[csr_matrix] = []\n",
    "\n",
    "print(\"\\n--- Starting Max Aggregation for All Clusters ---\")\n",
    "\n",
    "for k_index in range(K_CLUSTER):\n",
    "    raw_data_list = W_data[k_index]\n",
    "    row_list_k    = W_row[k_index]\n",
    "    col_list_k    = W_col[k_index]\n",
    "\n",
    "    if not raw_data_list:\n",
    "        # No edges in this cluster: just push an empty matrix\n",
    "        empty_csr = csr_matrix((MATRIX_DIM, MATRIX_DIM), dtype=np.float32)\n",
    "        Final_Aggregated_W.append(empty_csr)\n",
    "        print(f\"‚úÖ Final W_{k_index + 1}: EMPTY matrix, Shape={empty_csr.shape}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        data_array_k = np.asarray(raw_data_list, dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR in W_{k_index + 1} data conversion: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Call aggregation: note MATRIX_DIM = NUM_NODES + 1\n",
    "    Final_W_k_CSR = finalize_relation_matrix(\n",
    "        data_array_k.tolist(),\n",
    "        row_list_k,\n",
    "        col_list_k,\n",
    "        MATRIX_DIM\n",
    "    )\n",
    "\n",
    "    Final_Aggregated_W.append(Final_W_k_CSR)\n",
    "\n",
    "    print(f\"‚úÖ Final W_{k_index + 1}: \"\n",
    "          f\"Shape={Final_W_k_CSR.shape}, \"\n",
    "          f\"Unique Edges={Final_W_k_CSR.nnz}, \"\n",
    "          f\"Total Weight={Final_W_k_CSR.data.sum():.4f}\")\n",
    "\n",
    "print(f\"\\nSuccessfully created {len(Final_Aggregated_W)} final, maximized CSR matrices.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2690fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "print(\"\\n--- Final Maximized Relational Matrices (W_k) ---\")\n",
    "print(f\"Total Matrices in List: {len(Final_Aggregated_W)}\\n\")\n",
    "\n",
    "for k_index, final_Wk in enumerate(Final_Aggregated_W):\n",
    "    cluster_id = k_index + 1\n",
    "    print()\n",
    "    print(f\"**Matrix W_{cluster_id} (Cluster {cluster_id})**\")\n",
    "    print(f\"Shape: {final_Wk.shape}, \"\n",
    "          f\"Unique Edges (nnz): {final_Wk.nnz}, \"\n",
    "          f\"Total Weight: {final_Wk.data.sum():.4f}\")\n",
    "\n",
    "    if final_Wk.nnz == 0:\n",
    "        print(\"  [Matrix is empty (no relationships in this cluster)]\")\n",
    "        continue\n",
    "\n",
    "    coo_wk: coo_matrix = final_Wk.tocoo()\n",
    "\n",
    "    print(\"  Non-Zero Edges (Subject Index -> Object Index | Max Confidence Score):\")\n",
    "    for r, c, d in zip(coo_wk.row, coo_wk.col, coo_wk.data):\n",
    "        print(f\"  ({r} -> {c}) | Score: {d:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(Final_Aggregated_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c70ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_w = Final_Aggregated_W\n",
    "N = N\n",
    "T_map = T_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83278ae8",
   "metadata": {},
   "source": [
    "# 3.3 $C_{prior}$ generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e15d67",
   "metadata": {},
   "source": [
    "The goal here is to create a sparse answer key $C_{prior}$ (Our causal prior).\n",
    "\n",
    "Our Cprior is kinda a high-quality ‚Äùtraining dataset‚Äù that tlls us the true causal links in the document.\n",
    "Since we have no human labels, we must generate the dataset ourselves. This is a self-supervised process.\n",
    "The core idea is to use a large, powerful ‚ÄùTeacher‚Äù LLM (e.g., gemini-2.5-flash) to perform complex causal reasoning.\n",
    "The output of this phase is the Cprior (Causal Prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a38f4b2",
   "metadata": {},
   "source": [
    "## Active Candidate-Set Expansion (ACE)\n",
    "\n",
    "We cant query LLM with all pairs of nodes: O($N^2$).. So prune them to a list of plausible candidate pairs $E_{prior}$. This small list is what we send to our LLM.. This is done in 2 stages:\n",
    "1. Structural Filter (GAE)\n",
    "2. Semantic Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19513221",
   "metadata": {},
   "source": [
    "### 1. Structural Filter (GAE):\n",
    "\n",
    "Our initial A_w are myopic (They only contain local 1-hop links within a paragraph). We need an unsupervised way to find pairs (i, j) that are strongly connected via multi-hop paths, as these are highly plausible for causal relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, coo_matrix, hstack, vstack\n",
    "from typing import List\n",
    "\n",
    "# Assuming W_k_final_matrices is the list of K final, maximized CSR matrices.\n",
    "# We also assume that NUM_NODES is the correct dimension (e.g., 14 or 15).\n",
    "\n",
    "def create_co_occurrence_matrix(W_k_matrices: List[csr_matrix], N: int) -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Creates the binary, symmetric co-occurrence scaffolding graph (A_co-occur).\n",
    "\n",
    "    Args:\n",
    "        W_k_matrices: List of K aggregated relational matrices (W_1 to W_K).\n",
    "        N: The dimension N (total number of nodes).\n",
    "\n",
    "    Returns:\n",
    "        The A_co-occur matrix (N x N, binary, symmetric).\n",
    "    \"\"\"\n",
    "    if not W_k_matrices:\n",
    "        print(\"Warning: Input matrix list is empty. Returning empty N x N matrix.\")\n",
    "        return csr_matrix((N, N), dtype=np.int8)\n",
    "\n",
    "    print(f\"Starting creation of A_co-occur (N={N})...\")\n",
    "    \n",
    "    # 1. Binary Union (Logical OR)\n",
    "    # The sum of all W_k matrices performs a union of all non-zero entries.\n",
    "    # We initialize the union matrix with the first W_k matrix.\n",
    "    A_union = W_k_matrices[0].copy()\n",
    "    \n",
    "    for Wk in W_k_matrices[1:]:\n",
    "        # Standard sparse matrix addition combines all non-zero positions.\n",
    "        A_union = A_union + Wk\n",
    "\n",
    "    # 2. Binarization\n",
    "    # Since any Wk(i, j) > 0 satisfies the condition, we convert all positive values to 1.\n",
    "    A_binary = A_union.sign()\n",
    "    \n",
    "    # 3. Symmetrization (A + A^T)\n",
    "    # This ensures that if a directed relation (i -> j) exists, the undirected\n",
    "    # co-occurrence (i, j) is recorded regardless of direction.\n",
    "    A_transpose = A_binary.transpose()\n",
    "    A_symmetric_union = A_binary + A_transpose\n",
    "\n",
    "    # 4. Final Binarization (Handles values like 2, which result from A(i,j) + A(j,i))\n",
    "    A_co_occur = A_symmetric_union.sign().astype(np.int8)\n",
    "    \n",
    "    print(f\"‚úÖ A_co-occur created. Entries={A_co_occur.nnz}, Symmetrization successful.\")\n",
    "    return A_co_occur\n",
    "\n",
    "# --- Example Usage (Assuming variables are available) ---\n",
    "\n",
    "# Note: You must ensure NUM_NODES is the correct total dimension (N)\n",
    "A_co_occur = create_co_occurrence_matrix(Final_Aggregated_W, NUM_NODES)\n",
    "print(f\"Final A_co-occur Shape: {A_co_occur.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77428b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A_co_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ff76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the above A_co_occur\n",
    "\n",
    "import os\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(\"saved_stuff\", exist_ok=True)\n",
    "\n",
    "save_path = \"saved_stuff/A_co_occur.npz\"\n",
    "\n",
    "# A_co_occur is your csr_matrix\n",
    "save_npz(save_path, A_co_occur)\n",
    "\n",
    "print(f\"Saved A_co_occur sparse matrix to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7130662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import identity, csr_matrix\n",
    "\n",
    "# Assuming A_co_occur is the final CSR matrix generated previously.\n",
    "# If A_co_occur is not defined, this code assumes it has been loaded or created.\n",
    "\n",
    "def calculate_A_hat(A_co_occur: csr_matrix) -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Calculates the normalized adjacency matrix with self-loops, A_hat.\n",
    "    A_hat = A_co_occur + I, where I is the Identity Matrix.\n",
    "    \n",
    "    Args:\n",
    "        A_co_occur: The unweighted, symmetric co-occurrence matrix (N x N).\n",
    "        \n",
    "    Returns:\n",
    "        The A_hat matrix in CSR format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get the dimension N\n",
    "    N = A_co_occur.shape[0]\n",
    "    \n",
    "    # 2. Create the Identity Matrix I (in sparse format)\n",
    "    # The identity matrix is N x N, with ones on the diagonal.\n",
    "    I = identity(N, dtype=A_co_occur.dtype, format='csr')\n",
    "    \n",
    "    # 3. Calculate A_hat = A_co-occur + I\n",
    "    # Sparse matrix addition handles the union of the two sets of non-zero entries.\n",
    "    A_hat = A_co_occur + I\n",
    "    \n",
    "    # Ensure the result is still binary (0 or 1), although A_co_occur should not have values > 1.\n",
    "    # The sum of 1 (from A_co-occur) and 1 (from I) on the diagonal will result in 2.\n",
    "    # We must ensure the result is clipped back to 1.\n",
    "    A_hat_binary = A_hat.sign()\n",
    "    \n",
    "    # Convert back to the desired CSR integer format\n",
    "    return A_hat_binary.astype(np.int8)\n",
    "\n",
    "# --- Example Usage (Assuming A_co_occur is available) ---\n",
    "A_hat = calculate_A_hat(A_co_occur)\n",
    "\n",
    "print(f\"‚úÖ A_hat successfully created.\")\n",
    "print(f\"A_hat Shape: {A_hat.shape}\")\n",
    "print(f\"Non-Zero Entries (nnz): {A_hat.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13320ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from typing import List\n",
    "\n",
    "def calculate_D_hat(A_hat: csr_matrix) -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Calculates the diagonal degree matrix D_hat from A_hat.\n",
    "    \n",
    "    Fix: Ensures the degree vector is explicitly flattened to 1D before diags().\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculate Row Sums (The Degree Vector)\n",
    "    # A_hat.sum(axis=1) returns a dense numpy.matrix of shape (N, 1).\n",
    "    sum_matrix = A_hat.sum(axis=1)\n",
    "    \n",
    "    # 2. Flatten the result into a true 1D NumPy array (shape (N,))\n",
    "    # The .A1 attribute is the most robust way to flatten the result of a sparse matrix sum.\n",
    "    degree_vector = sum_matrix.A1 \n",
    "    \n",
    "    # 3. Construct the Diagonal Matrix D_hat\n",
    "    # diags now receives a proper 1D array for the diagonal.\n",
    "    D_hat = diags(degree_vector, format='csr')\n",
    "    \n",
    "    print(\"Degree Vector Shape (N):\", degree_vector.shape)\n",
    "    print(f\"D_hat diagonal values (first 5): {degree_vector[:5].tolist()}\")\n",
    "    \n",
    "    return D_hat\n",
    "\n",
    "# --- Example Usage ---\n",
    "D_hat = calculate_D_hat(A_hat)\n",
    "\n",
    "print(f\"‚úÖ D_hat matrix created.\")\n",
    "print(f\"D_hat Shape: {D_hat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4216eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(N)\n",
    "\n",
    "# We ideally also need the N0 node (which is just a 0 vector) -> Useful for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# 1. ADD N0 to the dictionary N\n",
    "N['N0'] = 'PLACEHOLDER_ZERO_INDEX'\n",
    "print(f\"Total nodes in N dictionary (N0 to N14): {len(N)}\")\n",
    "\n",
    "\n",
    "# 2. Prepare Data\n",
    "# Sort keys naturally: N0, N1, N2, ..., N10, N11, ...\n",
    "# N0 will be sorted to the front because its key lambda evaluates to 0.\n",
    "sorted_node_keys = sorted(N.keys(), key=lambda x: int(x[1:]) if x != 'N0' else 0)\n",
    "node_names = [N[key] for key in sorted_node_keys]\n",
    "\n",
    "# 3. Load Pretrained Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"Loaded embedding model. Vector dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "print(\"Starting node name embedding...\")\n",
    "\n",
    "# 4. Generate Embeddings (v_node)\n",
    "# This processes all 15 node names. N0's vector is at index 0.\n",
    "node_embeddings = model.encode(node_names, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "# --- Final Step: Overwrite N0 Embedding with Zero Vector ---\n",
    "# The N0 placeholder is guaranteed to be at index 0 due to the sorting key.\n",
    "node_embeddings[0, :] = 0.0\n",
    "\n",
    "print(f\"\\n‚úÖ N0 ('{N['N0']}') embedding vector successfully set to zero.\")\n",
    "\n",
    "# 5. Store and Verify\n",
    "print(\"\\n--- Node Embedding Results ---\")\n",
    "print(f\"Total number of nodes (N): {len(node_names)}\")\n",
    "print(f\"Embedding Matrix Shape: {node_embeddings.shape}\") \n",
    "\n",
    "NodeEmbeddingMap: Dict[str, np.ndarray] = {\n",
    "    key: node_embeddings[i] for i, key in enumerate(sorted_node_keys)\n",
    "}\n",
    "\n",
    "# Verification print\n",
    "print(f\"Verification: N0 vector sum is {np.sum(NodeEmbeddingMap['N0']):.4f}\")\n",
    "print(f\"Example Embedding (N1: {N[sorted_node_keys[1]]}):\")\n",
    "print(NodeEmbeddingMap['N1'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üö® FIX: Custom printing loop to show the dictionary keys in numerical order\n",
    "print(\"\\n--- NodeEmbeddingMap (Displayed in Correct Numerical Order) ---\")\n",
    "\n",
    "# We reuse the numerically sorted keys list to print the map\n",
    "for key in sorted_node_keys:\n",
    "    # Print the key, and a snippet of the vector\n",
    "    vector_snippet = NodeEmbeddingMap[key][:3]\n",
    "    print(f\"'{key}': array({vector_snippet.tolist()}, ...),\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabf33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Assuming NodeEmbeddingMap is defined and populated with your 15 node vectors.\n",
    "\n",
    "# --- Helper Function for Numerical Sorting ---\n",
    "def get_node_index(key: str) -> int:\n",
    "    \"\"\"Extracts the numerical suffix (e.g., 10 from N10) for sorting.\"\"\"\n",
    "    if key == 'N0':\n",
    "        return 0\n",
    "    # Uses int() casting to ensure N10 is treated as 10, not 1, for sorting\n",
    "    return int(key[1:])\n",
    "\n",
    "# 1. Prepare Keys in Numerical Order\n",
    "# This ensures N0 is row 0, N1 is row 1, N14 is row 14, aligning with matrix indices.\n",
    "sorted_keys_numeric = sorted(NodeEmbeddingMap.keys(), key=get_node_index)\n",
    "\n",
    "# 2. Construct the Matrix X by Vertical Stacking\n",
    "# We retrieve the vector corresponding to each sorted key and stack them vertically.\n",
    "X = np.vstack([NodeEmbeddingMap[key] for key in sorted_keys_numeric])\n",
    "\n",
    "# 3. Verification\n",
    "print(\"--- Final Node Feature Matrix (X) ---\")\n",
    "print(f\"‚úÖ Matrix X successfully created.\")\n",
    "print(f\"Shape N x d_in: {X.shape}\") \n",
    "# Expected shape: (15, 384)\n",
    "print(f\"Data Type: {X.dtype}\") \n",
    "\n",
    "# Verification of the N0 row (should be all zeros):\n",
    "# print(f\"Row 0 (N0 Placeholder) sum: {np.sum(X[0]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f701c4",
   "metadata": {},
   "source": [
    "Pre-computation and GAE Model Definition:\n",
    "\n",
    "This section defines the helper function to calculate the normalized adjacency matrix $\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$ and the PyTorch module for the GAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Hyperparameters and Dimensions ---\n",
    "N_NODES = 15\n",
    "D_IN = 384\n",
    "D_HIDDEN = 128\n",
    "D_LATENT = 64\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 5e-4\n",
    "NUM_EPOCHS = 500   # 500 is enough to see behaviour\n",
    "\n",
    "\n",
    "# --- 1. Model Definitions ---\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    \"\"\"Two-layer GCN encoder for GAE.\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels, cached=True)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels, cached=True)\n",
    "\n",
    "        # Optional: explicit Glorot init (PyG already uses good defaults)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- 2. Data Preparation (WITH FEATURE SCALING) ---\n",
    "\n",
    "def prepare_pyg_data(X_np: np.ndarray, A_cooccur_csr: csr_matrix):\n",
    "    \"\"\"\n",
    "    Converts NumPy/SciPy data into the PyTorch Geometric Data object,\n",
    "    applying necessary feature scaling (StandardScaler) for stability.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scale node features\n",
    "    scaler = StandardScaler()\n",
    "    X_np_scaled = scaler.fit_transform(X_np)\n",
    "\n",
    "    # ---- IMPORTANT: Make adjacency symmetric & remove self loops ----\n",
    "    A = A_cooccur_csr\n",
    "    # Symmetrize: A_undirected = A OR A^T\n",
    "    A_sym = ((A + A.T) > 0).astype(np.float32)\n",
    "\n",
    "    # Let GCNConv add self-loops itself; don't add I here\n",
    "    # Use PyG helper to create edge_index\n",
    "    edge_index, _ = from_scipy_sparse_matrix(A_sym)\n",
    "\n",
    "    x = torch.from_numpy(X_np_scaled).float()\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# --- 3. Training Function ---\n",
    "\n",
    "def train_pyg_gae(data: Data, epochs: int):\n",
    "    \n",
    "    encoder = GCNEncoder(\n",
    "        in_channels=D_IN,\n",
    "        hidden_channels=D_HIDDEN,\n",
    "        out_channels=D_LATENT\n",
    "    )\n",
    "    model = GAE(encoder)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=LEARNING_RATE,\n",
    "                           weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    print(f\"Training GAE for {epochs} epochs (lr={LEARNING_RATE}, decay={WEIGHT_DECAY})...\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "\n",
    "        # recon_loss expects a positive edge_index of the (unweighted) graph\n",
    "        loss = model.recon_loss(z, data.edge_index)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:03d}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Return final embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "# --- 4. Execution Block ---\n",
    "\n",
    "# Option A: your original random graph (will not give meaningful loss trend)\n",
    "X_np_sim = np.random.rand(N_NODES, D_IN).astype(np.float32)\n",
    "A_cooccur_sim = csr_matrix(\n",
    "    np.random.randint(0, 2, size=(N_NODES, N_NODES), dtype=np.int8)\n",
    ")\n",
    "\n",
    "# Make sure nonzero entries are 1\n",
    "A_cooccur_sim.data[:] = 1\n",
    "\n",
    "pyg_data = prepare_pyg_data(X_np_sim, A_cooccur_sim)\n",
    "\n",
    "Final_Embeddings_Z = train_pyg_gae(pyg_data, epochs=NUM_EPOCHS)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"‚úÖ PyG GAE Training Successfully Executed.\")\n",
    "print(f\"Final Latent Embedding Matrix Z Shape: {Final_Embeddings_Z.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aadce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = Final_Embeddings_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f253acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the above\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Make sure directory exists\n",
    "os.makedirs(\"saved_stuff\", exist_ok=True)\n",
    "\n",
    "save_path = \"saved_stuff/Z.pt\"\n",
    "\n",
    "# Z = Final_Embeddings_Z  # Ensure Z exists\n",
    "torch.save(Z, save_path)\n",
    "\n",
    "print(f\"Saved GAE embeddings Z to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Tuple, Set, Dict, Any\n",
    "# Assuming Final_Embeddings_Z is defined from the successful PyG training run.\n",
    "\n",
    "# --- Define Hyperparameters ---\n",
    "SCALING_RATIO = 0.30\n",
    "\n",
    "# --- Execution of ANN and Candidate Generation ---\n",
    "\n",
    "# 1. Prepare Data and Dimensions\n",
    "Z_np = Final_Embeddings_Z.cpu().numpy().astype('float32')\n",
    "N_NODES_ACTUAL = Z_np.shape[0] # N (Total number of nodes, e.g., 15)\n",
    "D_LATENT = Z_np.shape[1] \n",
    "\n",
    "# 2. Calculate Dynamic K_PRIME\n",
    "K_PRIME_SCALED = int(N_NODES_ACTUAL * SCALING_RATIO) \n",
    "K_PRIME = max(1, min(K_PRIME_SCALED, N_NODES_ACTUAL - 1))\n",
    "\n",
    "# 3. Build the ANN Index (FAISS)\n",
    "print(f\"Total Nodes (N): {N_NODES_ACTUAL}, K': {K_PRIME}\")\n",
    "print(\"Building FAISS Index...\")\n",
    "index = faiss.IndexFlatL2(D_LATENT) \n",
    "index.add(Z_np) \n",
    "\n",
    "# 4. Query the Index\n",
    "D, I = index.search(Z_np, K_PRIME + 1) \n",
    "\n",
    "# 5. Generate Candidate Set C1\n",
    "C1: Set[Tuple[int, int]] = set()\n",
    "raw_candidate_list: List[Dict[str, Any]] = []\n",
    "\n",
    "print(\"Populating Candidate Set C1, excluding N0 (index 0)...\")\n",
    "\n",
    "for i in range(N_NODES_ACTUAL):\n",
    "    neighbors_i = I[i]\n",
    "    \n",
    "    # We iterate over the indices starting from 1 (skipping index 0, which is node i)\n",
    "    for neighbor_idx in neighbors_i[1:]: \n",
    "        j = neighbor_idx\n",
    "        \n",
    "        # üö® FIX: Exclude any pair involving the placeholder node (index 0)\n",
    "        if i == 0 or j == 0:\n",
    "            continue\n",
    "            \n",
    "        if i == j: # Should not happen after skipping neighbors_i[0]\n",
    "            continue\n",
    "            \n",
    "        pair = (i, j)\n",
    "        \n",
    "        if pair not in C1:\n",
    "            C1.add(pair)\n",
    "            \n",
    "            # Find the position of j within the neighbors_i array to get D[i, position]\n",
    "            # Use a robust way to find the index position for the distance matrix D\n",
    "            pos_in_search = np.where(neighbors_i == j)[0][0]\n",
    "            \n",
    "            raw_candidate_list.append({\n",
    "                \"source_idx\": i,\n",
    "                \"target_idx\": j,\n",
    "                \"distance\": D[i, pos_in_search]\n",
    "            })\n",
    "\n",
    "# --- Final Verification and Output ---\n",
    "\n",
    "print(\"\\n--- Candidate Set C1 Results ---\")\n",
    "print(f\"Total Unique Nodes (N): {N_NODES_ACTUAL}\")\n",
    "print(f\"k' (Neighbors Searched): {K_PRIME}\")\n",
    "print(f\"Size of Candidate Set C1 (|C1|): {len(C1)} pairs\")\n",
    "\n",
    "# Display a snippet of the found candidates\n",
    "print(\"\\nSnippet of Candidates:\")\n",
    "for candidate in raw_candidate_list:\n",
    "    distance_score = np.sqrt(candidate['distance']).item() \n",
    "    print(f\"  {candidate['source_idx']} -> {candidate['target_idx']} | Dist: {distance_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(raw_candidate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "K1: List[Tuple[int, int]] = []\n",
    "\n",
    "for candidate in raw_candidate_list:\n",
    "    # Extract the source and target indices and store as a tuple (i, j)\n",
    "    source = candidate['source_idx']\n",
    "    target = candidate['target_idx']\n",
    "    \n",
    "    K1.append((source, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(K1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd31f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save K1 to the saved stuff directory\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"saved_stuff\", exist_ok=True)\n",
    "\n",
    "# Convert np.int64 ‚Üí int for JSON serialization\n",
    "def convert_k1_to_jsonable(K1):\n",
    "    return [(int(a), int(b)) for (a, b) in K1]\n",
    "\n",
    "jsonable_K1 = convert_k1_to_jsonable(K1)\n",
    "\n",
    "save_path = \"saved_stuff/k1.json\"\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(jsonable_K1, f, indent=2)\n",
    "\n",
    "print(f\"Saved K1 to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223b219",
   "metadata": {},
   "source": [
    "### 2. Semantic Filter \n",
    "\n",
    "Here our basic goal is to filter our K1 \"structurally plausible set\" down to a small K2 \"semantically plausible set\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6050ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(sentences_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93ace0",
   "metadata": {},
   "source": [
    "Our interesting RAG-HyDe-RAV pipeline is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45102bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from typing import List, Tuple, Set, Dict, Any, Optional\n",
    "\n",
    "\n",
    "\n",
    "# External Libraries required: google-genai, sentence-transformers, faiss-cpu, numpy, scikit-learn\n",
    "from google.genai import Client, types\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "# --- HYPERPARAMETERS ---\n",
    "K_HYPOTHETICAL = 3      # Number of causal hypotheses to generate per pair\n",
    "R_STOCHASTIC_SAMPLES = 5 # Number of samples for Semantic Entropy estimation\n",
    "K_RAG = 3               # Top-k snippets retrieved for verification\n",
    "K_BASE = 5              # Base pool size for RAG-MMR\n",
    "K_EXPANSION = 5         # Pool expansion factor for uncertainty\n",
    "LAMBDA_MMR = 0.5        # MMR trade-off: 0.5 favors both relevance and diversity\n",
    "KAPPA_RRF = 60          # RRF constant\n",
    "TAU_SUPPORT = 0.80      # Min confidence (p_support) threshold for verification\n",
    "TAU_ENTROPY = 0.15      # Max Semantic Entropy threshold (low entropy = stable)\n",
    "DELAY_SECONDS = 1        # Delay between API calls for stability (Can be 15s for free tier)\n",
    "LLM_MODEL = 'gemini-2.5-flash'\n",
    "\n",
    "\n",
    "# --- ASSUMED INPUTS (Must be defined in the execution environment) ---\n",
    "# N (Dict[str, str]): Node ID to Node Name map (e.g., {'N1': '$500M', ...})\n",
    "# C1 (Set[Tuple[int, int]]): The candidate set of 0-indexed plausible pairs (i, j)\n",
    "# sentences_list (List[str]): The source document broken into sentences\n",
    "# Z_np (np.ndarray): The N x dz latent embedding matrix from GAE/ANN\n",
    "# node_to_idx (Dict[str, int]): Map from N1 -> 1, N2 -> 2, etc. (for coordinate lookup)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Initialize Client and Embedder\n",
    "client = Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# ====================================================================\n",
    "# PHASE 1: PREPARATION & INDEXING\n",
    "# ====================================================================\n",
    "\n",
    "\n",
    "def setup_document_index(sentences: List[str]) -> Tuple[faiss.IndexFlatL2, np.ndarray]:\n",
    "    \"\"\"\n",
    "    VD = Document Vector Store, idx_doc = FAISS index built on VD.\n",
    "    \"\"\"\n",
    "    print(\"1. Embedding document sentences (VD)...\")\n",
    "    VD = embedder.encode(sentences, convert_to_numpy=True)\n",
    "\n",
    "    # Build ANN Index (idx_doc)\n",
    "    d_embed = VD.shape[1]\n",
    "    idx_doc = faiss.IndexFlatL2(d_embed)\n",
    "    idx_doc.add(VD)\n",
    "    print(f\"2. FAISS Index built on M={len(sentences)} sentences.\")\n",
    "\n",
    "    return idx_doc, VD\n",
    "\n",
    "\n",
    "# Pre-run the setup (Requires sentences_list to be defined)\n",
    "# idx_doc, VD = setup_document_index(sentences_list)\n",
    "\n",
    "# ====================================================================\n",
    "# PHASE 2: LLM CALLS AND UNCERTAINTY ESTIMATION\n",
    "# ====================================================================\n",
    "\n",
    "def llm_generate_hypotheses(node_i_name: str, node_j_name: str) -> List[str]:\n",
    "    \"\"\"f_hypothetical: Generates k_hypothetical causal claims.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Generate {K_HYPOTHETICAL} short, hypothetical sentences describing a plausible causal connection\n",
    "        between '{node_i_name}' (Cause) and '{node_j_name}' (Effect). Start each sentence with 'It is plausible that'.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=LLM_MODEL,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.8\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # print(f\"response = {response}\")\n",
    "    # --- Safely extract text from response ---\n",
    "    raw_text = \"\"\n",
    "\n",
    "    # Preferred: if response.text exists and is non-empty\n",
    "    raw_text = response.text\n",
    "    # print(f\"raw_text = {raw_text}\")\n",
    "\n",
    "    if not raw_text.strip():\n",
    "        print(\"[llm_generate_hypotheses] Empty LLM response; returning no hypotheses.\")\n",
    "        return []\n",
    "\n",
    "    # Simple heuristic to parse the generated sentences\n",
    "    hypotheses = [\n",
    "        line.strip()\n",
    "        for line in raw_text.split(\"\\n\")\n",
    "        if line.strip() and \"It is plausible that\" in line\n",
    "    ]\n",
    "\n",
    "    # pprint.pprint(hypotheses)\n",
    "    return hypotheses[:K_HYPOTHETICAL]\n",
    "\n",
    "\n",
    "class VerifySupport(BaseModel):\n",
    "    \"\"\"Schema for claim verification result.\"\"\"\n",
    "    support: Literal[\"YES\", \"NO\"]\n",
    "\n",
    "\n",
    "def llm_verify_claim(claim: str, evidence: str, temperature: float) -> Tuple[float, str]:\n",
    "    \"\"\"f_verify: Verifies factual support and returns confidence + the model's primary choice.\"\"\"\n",
    "\n",
    "    prompt_verify = f\"\"\"\n",
    "        Claim: '{claim}'.\n",
    "        Evidence: '{evidence}'.\n",
    "        Based ONLY on the Evidence, does the Evidence strongly support the Claim?\n",
    "    \"\"\"\n",
    "\n",
    "    # üîí Let Gemini enforce the schema for us\n",
    "    response = client.models.generate_content(\n",
    "        model=LLM_MODEL,\n",
    "        contents=prompt_verify,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=temperature,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=VerifySupport,  # <- Pydantic model, not raw dict\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    pprint.pprint(response)\n",
    "    choice = \"NO\"\n",
    "\n",
    "    try:\n",
    "        print(\"checkpoint 1\")\n",
    "        parsed = getattr(response, \"parsed\", None)\n",
    "        print(\"checkpoint 2, parsed =\", parsed)\n",
    "\n",
    "        support_val = None\n",
    "\n",
    "        # A) Best case: parsed is already a VerifySupport instance\n",
    "        if isinstance(parsed, VerifySupport):\n",
    "            print(\"checkpoint 3 (VerifySupport instance)\")\n",
    "            support_val = parsed.support\n",
    "\n",
    "        # Final sanity check\n",
    "        if support_val not in (\"YES\", \"NO\"):\n",
    "            raise ValueError(\"Could not find valid 'support' field in structured response\")\n",
    "\n",
    "        choice = support_val\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[llm_verify_claim] Failed to parse structured response, defaulting to NO: {e}\")\n",
    "        choice = \"NO\"\n",
    "\n",
    "    p_support = 1.0 if choice == \"YES\" else 0.0\n",
    "    return p_support, choice\n",
    "\n",
    "\n",
    "def estimate_semantic_entropy(claim: str, evidence: str) -> Tuple[float, float]:\n",
    "\n",
    "    \"\"\"Calculates p_support (mean confidence) and H_semantic via R stochastic passes.\"\"\"\n",
    "\n",
    "    # We use a moderate temperature for sampling (e.g., T=0.7)\n",
    "    TEMPERATURE = 0.7\n",
    "\n",
    "    # We will simply count the YES/NO responses over R passes\n",
    "    yes_count = 0\n",
    "\n",
    "    # Perform R stochastic forward passes\n",
    "    for _ in range(R_STOCHASTIC_SAMPLES):\n",
    "        p_support_r, choice_r = llm_verify_claim(claim, evidence, temperature=TEMPERATURE)\n",
    "        if choice_r == \"YES\":\n",
    "            yes_count += 1\n",
    "        time.sleep(DELAY_SECONDS) # Respect rate limits\n",
    "\n",
    "    # p_support: Average confidence\n",
    "    p_support_mean = yes_count / R_STOCHASTIC_SAMPLES\n",
    "\n",
    "    # H_semantic: Simple measure of consistency (1 - normalized variance)\n",
    "    # If all R samples agree, consistency is 1, entropy is low.\n",
    "    # We approximate consistency as the frequency of the majority vote.\n",
    "    consistency = max(p_support_mean, 1.0 - p_support_mean)\n",
    "\n",
    "    # H_semantic is low when consistency is high. We use 1 - consistency as a proxy for uncertainty.\n",
    "    h_semantic_proxy = 1.0 - consistency\n",
    "\n",
    "    return p_support_mean, h_semantic_proxy\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# PHASE 3: RAG-MMR AND RANK FUSION (RRF) LOGIC\n",
    "# ====================================================================\n",
    "\n",
    "def mmr_rerank(query_vec, candidates_vectors, k_rag: int) -> List[int]:\n",
    "    \"\"\"Reranks candidate indices using Maximal Marginal Relevance (MMR).\"\"\"\n",
    "\n",
    "    # Use global cosine similarity function (assuming it's defined or imported)\n",
    "    similarity = lambda a, b: cosine_similarity([a], [b])[0][0]\n",
    "\n",
    "    # Convert numpy array of vectors to list of vectors\n",
    "    candidates_list = [v.reshape(1, -1) for v in candidates_vectors]\n",
    "\n",
    "    if not candidates_list:\n",
    "        return []\n",
    "\n",
    "    selected_indices = []\n",
    "\n",
    "    for _ in range(min(k_rag, len(candidates_list))):\n",
    "        best_mmr_score = -np.inf\n",
    "        best_candidate_index = -1\n",
    "        for idx, d_i_vec in enumerate(candidates_list):\n",
    "            if idx in selected_indices:\n",
    "                continue\n",
    "            # Calculate Relevance: Sim(d_i, q)\n",
    "\n",
    "            rel_score = similarity(d_i_vec, query_vec)\n",
    "\n",
    "            # Calculate Diversity: max(Sim(d_i, d_j)) for already selected j\n",
    "\n",
    "            div_score = 0.0\n",
    "            if selected_indices:\n",
    "                selected_vectors = [candidates_vectors[j] for j in selected_indices]\n",
    "                # Calculate similarity of current candidate to all selected vectors\n",
    "                sim_to_selected = [similarity(d_i_vec, s_vec) for s_vec in selected_vectors]\n",
    "                div_score = max(sim_to_selected)\n",
    "\n",
    "            # MMR Formula: lambda * Relevance - (1 - lambda) * Diversity\n",
    "            mmr_score = (LAMBDA_MMR * rel_score) - ((1 - LAMBDA_MMR) * div_score)\n",
    "\n",
    "            if mmr_score > best_mmr_score:\n",
    "                best_mmr_score = mmr_score\n",
    "                best_candidate_index = idx\n",
    "\n",
    "        if best_candidate_index != -1:\n",
    "            selected_indices.append(best_candidate_index)\n",
    "\n",
    "    # Return the indices of the selected candidates in the pool\n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(all_ranked_lists: List[List[int]], k_final: int) -> List[int]:\n",
    "    \"\"\"Applies RRF to combine multiple ranked lists into a single consensus list.\"\"\"\n",
    "    RRF_scores = {}\n",
    "    for ranked_list in all_ranked_lists:\n",
    "        for rank, doc_index in enumerate(ranked_list, start=1):\n",
    "            score = 1.0 / (KAPPA_RRF + rank)\n",
    "            RRF_scores[doc_index] = RRF_scores.get(doc_index, 0.0) + score\n",
    "\n",
    "    # Sort by RRF Score and select top k_final\n",
    "    sorted_scores = sorted(RRF_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    return [doc_index for doc_index, score in sorted_scores][:k_final]\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# PHASE 4: MAIN PIPELINE EXECUTION\n",
    "# ====================================================================\n",
    "\n",
    "def run_causal_verification_pipeline(candidate_set: Set[Tuple[int, int]], N_map: Dict[str, str], sentences: List[str]) -> Dict[Tuple[int, int], Dict[str, Any]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Executes the full hypothesis verification and RAG-MMR pipeline for all candidate pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Setup ---\n",
    "    # Convert node indices (0-14) back to N-map keys for naming purposes\n",
    "    idx_to_node_name = {idx: name for idx, name in enumerate(sorted(N_map.keys(), key=lambda x: int(x[1:]) if x != 'N0' else 0))}\n",
    "\n",
    "    # 1. Prepare Document Index\n",
    "    idx_doc, VD = setup_document_index(sentences)\n",
    "    final_output_map: Dict[Tuple[int, int], Dict[str, Any]] = {}\n",
    "\n",
    "    # --- Main Loop: Iterate through each plausible link (i, j) ---\n",
    "    for i, j in candidate_set:\n",
    "        # Skip pairs involving the N0 placeholder (index 0)\n",
    "        if i == 0 or j == 0:\n",
    "            continue\n",
    "\n",
    "        node_i_name = N_map.get(idx_to_node_name.get(i, f'N{i}'), f'Node {i}')\n",
    "        node_j_name = N_map.get(idx_to_node_name.get(j, f'N{j}'), f'Node {j}')\n",
    "\n",
    "        print(f\"\\n--- Processing Pair: {node_i_name} ({i}) -> {node_j_name} ({j}) ---\")\n",
    "\n",
    "        # H_verified storage for this pair\n",
    "        H_verified_list: List[str] = []\n",
    "        all_ranked_lists_for_pair: List[List[int]] = []\n",
    "\n",
    "        # --- 1. Hypothesis Generation ---\n",
    "        hypotheses = llm_generate_hypotheses(node_i_name, node_j_name)\n",
    "        if not hypotheses:\n",
    "            print(\"Skipped: No hypotheses generated.\")\n",
    "            continue\n",
    "\n",
    "        # --- 2. Verification and Dual Filtering ---\n",
    "        for hl in hypotheses:\n",
    "            # 2a. Embed hypothesis and retrieve evidence\n",
    "            v_hl = embedder.encode([hl])[0].reshape(1, -1)\n",
    "            D_rag, I_rag = idx_doc.search(v_hl, K_RAG)\n",
    "            verification_snippets = \" \".join([sentences[idx] for idx in I_rag[0]])\n",
    "\n",
    "            # 2b. Estimate support and entropy (via R stochastic passes)\n",
    "            p_support, h_semantic = estimate_semantic_entropy(hl, verification_snippets)\n",
    "\n",
    "            # 2c. Dual Filtering: Check thresholds\n",
    "            if p_support > TAU_SUPPORT and h_semantic < TAU_ENTROPY:\n",
    "                H_verified_list.append(hl)\n",
    "\n",
    "                # --- 3. Adaptive Pooling and MMR (For verified hypotheses only) ---\n",
    "                score_h = p_support # Use p_support as the score for adaptive pooling\n",
    "\n",
    "                # Adaptive Pool Size: kpool = kbase + (1 - score) * kexpansion\n",
    "                k_pool = int(K_BASE + (1.0 - score_h) * K_EXPANSION)\n",
    "                k_pool = max(K_RAG, k_pool) # Ensure pool is at least k_RAG\n",
    "\n",
    "                # Retrieve large pool\n",
    "                D_pool, I_pool = idx_doc.search(v_hl, k_pool)\n",
    "                pool_vectors = VD[I_pool[0]]\n",
    "\n",
    "                # MMR Reranking\n",
    "                final_indices_h = mmr_rerank(v_hl, pool_vectors, k_rag=K_RAG)\n",
    "\n",
    "                # Store indices (mapped back to global sentence IDs)\n",
    "                global_indices_h = [I_pool[0][idx] for idx in final_indices_h]\n",
    "                all_ranked_lists_for_pair.append(global_indices_h)\n",
    "\n",
    "            print(f\"  Claim Verified: {p_support:.2f}/{TAU_SUPPORT:.2f} (Support) | {h_semantic:.2f}/{TAU_ENTROPY:.2f} (Entropy) -> {'KEPT' if hl in H_verified_list else 'REJECTED'}\")\n",
    "\n",
    "        # --- 4. Final Rank Fusion (RRF) ---\n",
    "        if all_ranked_lists_for_pair:\n",
    "            final_indices_total = reciprocal_rank_fusion(all_ranked_lists_for_pair, k_final=3) # Select top 3 snippets\n",
    "            evidence_text = \"\\n\".join([sentences[idx] for idx in final_indices_total])\n",
    "        else:\n",
    "            final_indices_total = []\n",
    "            evidence_text = \"No strong, stable evidence found.\"\n",
    "\n",
    "        # --- 5. Final Output Format ---\n",
    "        final_output_map[(i, j)] = {\n",
    "            \"verified_causal_hypothesis\": H_verified_list,\n",
    "            \"evidence_text\": evidence_text\n",
    "        }\n",
    "\n",
    "        time.sleep(DELAY_SECONDS) # Respect rate limits\n",
    "\n",
    "    return final_output_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import List, Dict, Tuple, Any, Set\n",
    "# Assuming all function definitions (setup_document_index, run_causal_verification_pipeline, etc.) are available in the environment.\n",
    "\n",
    "# --- Helper Function (Required for MMR Reranking) ---\n",
    "# MMR calculation requires cosine similarity, which we define here.\n",
    "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity\n",
    "# We use a wrapper function for consistency, as the previous logic relied on it\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    # Handles 1-dimensional array input (vector)\n",
    "    return sklearn_cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0][0]\n",
    "\n",
    "\n",
    "# 3. Candidate Set C1 (Simulated output from GAE/ANN process)\n",
    "# Using a small, representative sample of plausible pairs (0-indexed)\n",
    "C1: Set[Tuple[int, int]] = {\n",
    "    (4, 13), (13, 11), (12, 8), (12, 10), (10, 14), (3, 2), (12, 13), (6, 3), (2, 3), (1, 10)\n",
    "}\n",
    "\n",
    "# 4. Latent Embeddings (Simulated output from GAE)\n",
    "# This would be the Final_Embeddings_Z tensor, converted to NumPy.\n",
    "# D_LATENT = 64, N_NODES = 15.\n",
    "Z_np_sim = np.random.rand(15, 64).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28765d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SETUP: Embeddings and ANN Index\n",
    "# Get the Document Vector Store (VD) and the FAISS Index (idx_doc)\n",
    "# This needs to run once.\n",
    "idx_doc, VD = setup_document_index(sentences_list)\n",
    "\n",
    "\n",
    "# 2. MAIN EXECUTION: Run the Full Verification Pipeline\n",
    "# The pipeline integrates the LLM calls, RAG, Entropy, and RRF.\n",
    "final_causal_map = run_causal_verification_pipeline(\n",
    "    candidate_set=C1, \n",
    "    N_map=N, \n",
    "    sentences=sentences_list\n",
    ")\n",
    "\n",
    "# 3. FINAL OUTPUT\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL CAUSAL VERIFICATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "# Use pprint for clean output of the final JSON-like structure\n",
    "import pprint\n",
    "pprint.pprint(final_causal_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6a06f",
   "metadata": {},
   "source": [
    "### Important notice\n",
    "Now we are done till a decent level (Im skipping the part where we generate K2 pairs from K1 pairs as of now, lets take that later on ) As of now i'll save important things and move on to CoCAD dp and CausGT=HS model.\n",
    "\n",
    "Hence as of now K1 plausible candidate pairs are only my Eprior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu-env)",
   "language": "python",
   "name": "gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
